# Feature 3.1: LLM Service Architecture

## Summary

Feature 3.1 implements a robust LLM service that manages connections to multiple providers (OpenAI, Anthropic, etc.) with automatic fallback, circuit breaker patterns, rate limiting, and cost tracking.

## Architecture Overview

The LLM service is built with the following components:

### Core Service (`RubberDuck.LLM.Service`)
- GenServer that manages all LLM interactions
- Handles provider selection and fallback logic
- Manages request queuing and processing
- Implements health monitoring and cost tracking

### Provider System
- **Behaviour**: `RubberDuck.LLM.Provider` defines the interface all providers must implement
- **Implementations**:
  - `OpenAI`: Supports GPT-4, GPT-4 Turbo, GPT-3.5 Turbo
  - `Anthropic`: Supports Claude 3 Opus, Sonnet, and Haiku
  - `Mock`: For testing and development

### Supporting Modules
- **Request/Response**: Unified data structures for all providers
- **ProviderConfig**: Configuration management
- **ProviderState**: Runtime state tracking
- **CostTracker**: Usage and cost monitoring
- **HealthMonitor**: Provider health tracking

## Key Features Implemented

### 1. Multi-Provider Support
- Pluggable provider architecture
- Each provider implements the same behaviour
- Unified request/response format across providers

### 2. Circuit Breaker Pattern
- Prevents cascading failures
- Three states: closed (normal), open (failing), half-open (testing)
- Automatic recovery after cooldown period

### 3. Rate Limiting
- Token bucket algorithm using `ex_rated`
- Per-provider rate limits
- Request queuing when rate limited

### 4. Automatic Fallback
- Falls back to secondary providers on failure
- Priority-based provider selection
- Maintains model compatibility

### 5. Request Management
- Synchronous and asynchronous requests
- Request queuing with priority support
- Retry logic with exponential backoff

### 6. Cost Tracking
- Tracks token usage per request
- Calculates costs based on provider pricing
- Provides cost summaries by provider/model

### 7. Health Monitoring
- Regular health checks for each provider
- Uptime tracking and metrics
- Circuit breaker integration

## Configuration

The service is configured in `config/llm.exs`:

```elixir
config :rubber_duck, RubberDuck.LLM.Service,
  providers: [
    %{
      name: :openai,
      adapter: RubberDuck.LLM.Providers.OpenAI,
      api_key: System.get_env("OPENAI_API_KEY"),
      models: ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"],
      priority: 1,
      rate_limit: {100, :minute},
      max_retries: 3
    }
  ]
```

## Usage Examples

### Basic Completion
```elixir
{:ok, response} = RubberDuck.LLM.Service.completion(
  model: "gpt-4",
  messages: [
    %{"role" => "user", "content" => "Hello, world!"}
  ],
  temperature: 0.7
)
```

### Async Request
```elixir
{:ok, request_id} = RubberDuck.LLM.Service.completion_async(opts)
# ... do other work ...
{:ok, response} = RubberDuck.LLM.Service.get_result(request_id)
```

### Cost Tracking
```elixir
{:ok, summary} = RubberDuck.LLM.Service.cost_summary()
# Returns total costs, token usage, etc.
```

## Testing

The implementation includes:
- Unit tests for core service functionality
- Provider-specific tests
- Mock provider for isolated testing
- Response parsing tests
- Cost calculation tests

## Next Steps

This foundation enables:
- Adding more providers (local models, Cohere, etc.)
- Implementing streaming support
- Adding caching layer
- Building higher-level abstractions
- Integration with the engine system

## Technical Decisions

1. **GenServer over Phoenix.PubSub**: Direct process communication for lower latency
2. **Fuse for Circuit Breaker**: Well-tested library instead of custom implementation
3. **Unified Response Format**: Simplifies downstream processing
4. **Provider Behaviour**: Ensures consistency and enables easy extension
5. **Separate Config/State**: Clean separation of configuration and runtime state

## Benefits

- **Reliability**: Automatic failover and circuit breakers prevent outages
- **Cost Control**: Rate limiting and cost tracking prevent bill surprises
- **Flexibility**: Easy to add new providers or change configurations
- **Observability**: Health monitoring and telemetry integration
- **Developer Experience**: Consistent API regardless of provider