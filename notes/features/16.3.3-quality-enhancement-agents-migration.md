# Feature: Quality and Enhancement Agents Migration (16.3.3)

## Summary
Migrate QualityImprovementAgent and CorrectionStrategyAgent from BaseAgent/mixed patterns to pure Jido.Agent architecture with proper Action-based design.

## Requirements
- [ ] Remove BaseAgent dependency from QualityImprovementAgent and migrate to Jido.Agent
- [ ] Extract quality analysis logic into separate Jido Actions
- [ ] Remove BaseAgent dependency from CorrectionStrategyAgent and migrate to Jido.Agent  
- [ ] Extract strategy selection logic into separate Jido Actions
- [ ] Implement proper signal handling using Jido patterns
- [ ] Add learning and adaptation mechanisms for both agents
- [ ] Create validation and verification actions
- [ ] Ensure backward compatibility with existing signal interfaces
- [ ] Add comprehensive metrics tracking and reporting

## Research Summary

### Existing Usage Rules Checked
- Jido Agent patterns: Found in tools/agents using `use Jido.Agent` with Actions defined as separate modules
- Action patterns: Actions use `use Jido.Action, name: "action_name"` with parameter_schema and run/2
- Signal handling: Agents use handle_signal/2 for routing signals to appropriate Actions

### Documentation Reviewed
- Jido.Agent: Provides agent framework with state management, signal handling, and action execution
- Jido.Action: Provides action framework with parameter validation and execution context

### Existing Patterns Found
- Action definition pattern: lib/rubber_duck/tools/agents/security_analyzer_agent.ex:299 - Actions defined as nested modules
- Agent pattern: lib/rubber_duck/tools/agents/todo_extractor_agent.ex:8 - Pure Jido.Agent with actions list
- Signal routing: lib/rubber_duck/tools/agents/test_summarizer_agent.ex:1059 - handle_signal/2 routing to actions

### Technical Approach

1. **QualityImprovementAgent Migration**:
   - Convert from `use RubberDuck.Agents.BaseAgent` to `use Jido.Agent`
   - Extract quality analysis into `AnalyzeQualityAction`
   - Extract improvement application into `ApplyImprovementAction`
   - Extract standards enforcement into `EnforceStandardsAction`
   - Extract metrics tracking into `TrackMetricsAction`
   - Create reporting action `GenerateQualityReportAction`

2. **CorrectionStrategyAgent Migration**:
   - Convert from `use RubberDuck.Agents.BaseAgent` to `use Jido.Agent`
   - Extract strategy selection into `SelectStrategyAction`
   - Extract cost estimation into `EstimateCostAction`
   - Extract learning logic into `LearnFromOutcomeAction`
   - Extract A/B testing into `RunStrategyExperimentAction`
   - Create performance tracking `TrackStrategyPerformanceAction`

3. **Signal Handling Updates**:
   - Implement handle_signal/2 for both agents
   - Route signals to appropriate actions based on signal type
   - Maintain backward compatibility with existing signal formats

## Risks & Mitigations
| Risk | Impact | Mitigation |
|------|--------|------------|
| Breaking existing integrations | High | Maintain same signal interface, add compatibility layer if needed |
| Loss of state during migration | Medium | Carefully map existing state structure to Jido state |
| Performance degradation | Low | Profile before/after, optimize Action execution |
| Missing functionality | Medium | Comprehensive testing against existing test suite |

## Implementation Checklist
- [ ] Create new QualityImprovementAgent with Jido.Agent
- [ ] Define quality analysis Actions
- [ ] Define improvement Actions  
- [ ] Define metrics and reporting Actions
- [ ] Implement signal routing for quality agent
- [ ] Create new CorrectionStrategyAgent with Jido.Agent
- [ ] Define strategy selection Actions
- [ ] Define cost estimation Actions
- [ ] Define learning and experiment Actions
- [ ] Implement signal routing for correction agent
- [ ] Test both agents with existing test cases
- [ ] Verify no regressions in functionality
- [ ] Update documentation

## Questions for Pascal
1. Should we maintain the exact same state structure or optimize it for Jido patterns?
2. Are there any specific performance requirements for these agents?
3. Should the learning mechanisms be persistent across agent restarts?