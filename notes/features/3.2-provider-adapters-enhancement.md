# Feature: Provider Adapters Enhancement

## Summary
Enhance existing LLM provider adapters with streaming support, improved function calling, and accurate token counting capabilities.

## Requirements
- [x] Implement streaming response support for OpenAI provider
- [x] Implement streaming response support for Anthropic provider  
- [ ] Add function calling support with proper formatting for OpenAI
- [x] Improve token counting accuracy using proper tokenizers
- [x] Create streaming response parser for SSE format
- [x] Add streaming support to the LLM service layer
- [x] Ensure backward compatibility with non-streaming requests
- [x] Add comprehensive tests for streaming functionality

## Research Summary

### Existing Usage Rules Checked
- No specific usage rules found for streaming in the codebase
- Req library supports streaming via `:into` option

### Documentation Reviewed
- **OpenAI Streaming**: Uses Server-Sent Events (SSE) format with "data: " prefix, supports `stream_options` for usage stats
- **Anthropic Streaming**: Also uses SSE format, similar structure but different event types
- **Req Library**: Built-in streaming support with callbacks, collectables, or process mailbox

### Existing Patterns Found
- Provider behavior pattern already established: `lib/rubber_duck/llm/provider.ex`
- Basic streaming option already passed through request flow but not implemented
- Async request handling exists: `completion_async/1` in service

### Technical Approach

1. **Streaming Infrastructure**:
   - Create `RubberDuck.LLM.StreamParser` module for SSE parsing
   - Add `stream_response/2` callback to Provider behavior  
   - Implement streaming in OpenAI and Anthropic providers using Req's `:into` option

2. **Function Calling Enhancement**:
   - Extend OpenAI provider to properly format function definitions
   - Add response parsing for function call results
   - Create type specs for function schemas

3. **Token Counting**:
   - Research and integrate proper tokenizer libraries (tiktoken for OpenAI)
   - Implement accurate counting methods per model
   - Cache tokenizer instances for performance

4. **Service Layer Updates**:
   - Add `completion_stream/2` to LLM.Service
   - Handle streaming responses via GenServer messages
   - Provide callback mechanism for chunk processing

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| SSE parsing complexity | High | Use battle-tested parsing approach, extensive testing |
| Memory usage with long streams | Medium | Implement proper stream cleanup, timeouts |
| Breaking existing API | High | Keep non-streaming API unchanged, add new methods |
| Token counting accuracy | Low | Document approximation for unsupported models |

## Implementation Checklist
- [x] Create `lib/rubber_duck/llm/stream_parser.ex` for SSE parsing
- [x] Update Provider behavior with streaming callbacks
- [x] Enhance `lib/rubber_duck/llm/providers/openai.ex` with streaming
- [x] Enhance `lib/rubber_duck/llm/providers/anthropic.ex` with streaming
- [ ] Add function calling types and formatting to OpenAI provider
- [x] Research and add tokenizer dependencies to mix.exs
- [x] Implement accurate token counting per provider
- [x] Update LLM.Service with streaming methods
- [x] Create streaming response types
- [x] Write comprehensive tests for streaming
- [ ] Test function calling with complex schemas
- [x] Verify backward compatibility

## Questions for Zach
1. Do we want to expose streaming at the engine level or keep it internal to LLM service?
2. Should we support partial response caching during streaming?
3. Any preference on tokenizer libraries (pure Elixir vs NIFs)?
4. Do we need streaming timeout configuration per provider?

## Log
- Created feature branch: `feature/3.2-provider-adapters-enhancement`
- Set up todo tracking for implementation tasks
- ✅ Implemented streaming support for OpenAI and Anthropic providers
- ✅ Created unified StreamParser module for SSE parsing
- ✅ Added streaming methods to LLM.Service
- ✅ Implemented proper tokenization with tiktoken library
- ✅ Created comprehensive tokenization module with provider-specific counting
- ✅ Added dependencies: tiktoken ~> 0.4, tokenizers ~> 0.5
- ✅ All streaming and tokenization tests passing

## Tokenization Implementation Details

### Libraries Added
- **tiktoken ~> 0.4**: OpenAI's official tokenizer with Rust bindings
- **tokenizers ~> 0.5**: HuggingFace tokenizers for broader model support

### Features Implemented
- Accurate token counting for OpenAI models (GPT-4, GPT-3.5-turbo, GPT-4o)
- Character-based approximation for Claude models
- Message overhead calculation for different providers
- Graceful fallback to approximation when tiktoken fails
- Support for both text and message arrays
- Provider detection based on model names

### Usage
```elixir
# Count tokens for text
{:ok, count} = Tokenization.count_tokens("Hello world", "gpt-4")

# Count tokens for messages (includes overhead)
messages = [%{"role" => "user", "content" => "Hello"}]
{:ok, count} = Tokenization.count_tokens(messages, "gpt-4")

# Get encoding information
encoding = Tokenization.get_encoding_for_model("gpt-4")  # "cl100k_base"
```