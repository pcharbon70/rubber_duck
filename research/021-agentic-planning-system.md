
# Agent-Based Planning System for RubberDuck (Jido + Spark DSL)

This document outlines a complete implementation strategy for upgrading the RubberDuck planning system into a fully agentic, modular, and declarative system using Jido and Spark DSL.

---

## Design Goals

- **Modularity**: Separate each planning function into a dedicated Jido agent.
- **Declarative Strategy**: Use a Spark DSL to define reusable planning strategies.
- **Dynamic Coordination**: Let agents communicate and refine plans through Jido.
- **System Integration**: Leverage existing Engine Manager, Tool DSL, and Workflow Orchestrator.

---

## Planning Agents and Responsibilities

### PlanCoordinatorAgent
- Central controller of the planning session.
- Loads appropriate Spark DSL template based on task type.
- Assigns planning subtasks to other agents.
- Enforces execution strategy and convergence.

### TaskDecomposerAgent
- Extracts task decomposition steps from DSL.
- Uses Engine Manager to generate subgoals from prompts.
- Builds dependency DAG of subtasks.

### SubtaskExecutorAgent
- Executes each subtask using LLMs or Tool DSL.
- Streams results and monitors progress.
- Publishes outcomes to critics.

### CriticAgent
- Evaluates outputs using syntax, test, and logic critics.
- Provides hard (blocking) or soft (advisory) feedback.
- Coordinates with RefinementAgent on failed tasks.

### RefinementAgent
- Revises plans based on critic feedback or strategy failure.
- Rewrites prompts, retries execution, or updates DAG.
- Works closely with PlanCoordinatorAgent to ensure progress.

---

## Spark DSL Planning Templates

Templates define:
- `applies_to` type (e.g. :bugfix, :feature)
- Steps with prompts and optional critics
- Execution strategy: sequential, DAG, reflective

Example:
```elixir
defplanning_template "feature_tdd" do
  applies_to :feature

  decomposition do
    step :write_spec, prompt: "Describe the user-visible behavior"
    step :write_tests, prompt: "Write failing tests"
    step :implement_feature, prompt: "Write code to pass the tests", critic: :test
    step :refactor, prompt: "Refactor code", critic: :style
  end

  strategy :reflective
end
```

---

## Execution Lifecycle

1. `PlanCoordinatorAgent` receives a plan request and selects a template.
2. `TaskDecomposerAgent` generates subgoals from the decomposition.
3. `SubtaskExecutorAgent` executes each subgoal.
4. `CriticAgent` validates each output using defined critics.
5. `RefinementAgent` steps in on failure and updates the DAG.
6. Final validated plan is submitted to the Workflow Orchestrator.

---

## Jido-Based Agent Communication

Agents subscribe to topic-based messages via Jido:
- `:plan_request`, `:decomposition_ready`, `:subgoal_result`, `:critique_feedback`, `:refine_plan`
- Jido ensures coordination without tight coupling

---

## Supervision and Runtime

- Each agent is a `GenServer` under `PlanSupervisor`
- DSL templates cached and parsed at startup
- DAG and plan state stored in ETS or process context
- Supports retries, branching, and failure isolation

---

## System Integration

| Component            | Role                                      |
|---------------------|-------------------------------------------|
| Engine Manager       | Used by agents for all LLM operations     |
| Tool DSL             | Executes non-AI tools inside subtasks     |
| Workflow Orchestrator| Receives final executable Ash workflow    |
| Critics              | Intervene after key planning stages       |

---

## Benefits

- Modular planning agents (testable and extensible)
- Reusable declarative planning templates
- Autonomy with safety via critics and retries
- Seamless Elixir-native integration with RubberDuck’s ecosystem

---

Agent-Based Planning System for RubberDuck (Jido Multi-Agent Design)
Overview
We propose a fully agentic planning loop for the RubberDuck coding assistant, where specialized Jido agents collaborate to generate and refine a plan from a user’s prompt. This design follows the LLM-Modulo approach: large language models (LLMs) produce an initial plan which is then evaluated and improved by “critics”. Each key planning phase – decomposition, subgoal planning, critique, and refinement – is handled by a dedicated agent. The agents communicate via Jido’s signal-based messaging (CloudEvents) in an autonomous loop until a high-quality plan emerges. This new system leverages RubberDuck’s existing architecture (Engine Manager, Tool DSL, Workflow Orchestrator) while improving modularity and fault-tolerance through OTP supervision.
Agent Roles and Responsibilities
Each planning component is implemented as its own Jido agent process. The agents have clearly defined roles but work collectively toward a complete plan:
PlanCoordinator Agent (Plan Manager)
The PlanCoordinator orchestrates the entire planning cycle. It receives the initial user prompt (and context) and is responsible for plan lifecycle management – from creation to completion. It selects or instantiates a planning template (from the Spark DSL) appropriate to the request and maintains the plan state (task list, statuses, and any constraints). The PlanCoordinator triggers each phase of planning in turn and aggregates results. It tracks progress and state transitions (e.g. from “decomposing” to “critiquing” to “refining”) and handles concurrency control to ensure agents work on the correct plan version. If multiple plans/users run in parallel, each PlanCoordinator instance isolates one plan’s state (with mechanisms like locks or plan-specific IDs to avoid cross-talk). The PlanCoordinator also decides when the plan is “good enough” to stop or if it should abort/rollback on irreconcilable issues. In summary, this agent is the central facilitator that kicks off planning, mediates agent interactions, and guarantees the loop converges or terminates.
PlanDecomposer Agent (Task Decomposition)
The PlanDecomposer is tasked with breaking down the high-level goal into an initial set of tasks and subgoals. It interprets the user’s request (and any template guidance) and produces a structured plan outline. This agent encapsulates the logic of the prior TaskDecomposer engine, now exposed via Jido signals. Using LLM reasoning (chain-of-thought integration) and predefined strategies, it can perform hierarchical decomposition – e.g. first splitting a complex feature into major steps, then further subtasking as needed. The PlanDecomposer may employ multiple strategies (linear list vs. tree-of-thought) depending on the problem complexity. It also identifies dependencies between tasks (orderings, prerequisites) and estimates complexity or resource needs for each subgoal. In operation, the PlanCoordinator sends a “plan.decompose” signal to this agent with the top-level goal; the PlanDecomposer then generates an initial task list (possibly consulting the Spark DSL template for any pre-defined steps or structure) and returns those tasks via a signal (e.g. plan.decomposed). Essentially, this agent provides the first draft plan, which other agents will refine. It focuses on what needs to be done (task names, descriptions, dependencies), deferring how to the execution phase or lower-level planning.
SubgoalGenerator Agent (Subtask Planner)
The SubgoalGenerator specializes in fine-grained subgoal expansion for tasks. Whereas the PlanDecomposer produces a high-level breakdown, the SubgoalGenerator ensures each task is actionable by generating any intermediate steps or details required to carry it out. For example, if the PlanDecomposer yields a task “Implement Feature X,” the SubgoalGenerator might elaborate sub-steps: “X.a: Design API”, “X.b: Write unit tests,” etc. This agent can be invoked on each task in turn (or on-demand for particularly complex tasks) to produce nested plans. In practice, the PlanCoordinator or PlanDecomposer will emit a signal like plan.expand_task for tasks that require further refinement; the SubgoalGenerator (which subscribes to such events) responds with a set of subgoals or an updated task definition. Internally it uses similar LLM-driven techniques to decompose tasks recursively, leveraging any task templates available in the Spark DSL for that context. By isolating subtask planning in its own agent, the system supports hierarchical planning: the top-level PlanDecomposer focuses on broad phases, while the SubgoalGenerator ensures no task is underspecified. This separation of concerns makes the planning more modular and allows reuse of the SubgoalGenerator for any task that needs extra detail. (In some cases, the PlanDecomposer agent itself might recursively spawn or call the SubgoalGenerator for deep tasks, but logically they are distinct roles.)
Critique Agents (Critics Coordinator & Critics)
Once a draft plan is available, one or more Critic agents evaluate its soundness and quality. The critique phase is handled by a team of agents under a CriticsCoordinator orchestrator. The CriticsCoordinator receives a signal (e.g. plan.ready_for_review) from the PlanCoordinator when a plan version is ready to be vetted. It then dispatches the plan to multiple specialized critic agents (or alternatively, broadcasts a critique request that all critics subscribe to). This design allows parallel, independent reviews of the plan. For example, we can implement distinct critic agents for structural validity, dependency consistency, completeness, feasibility, security implications, etc.. Each CriticAgent implements a common behavior (e.g. CriticBehavior) and focuses on one aspect:
Hard critics might enforce correctness and constraints (syntax validity, dependency graph cycle checks, resource feasibility),
Soft critics might check for quality and best practices (code style conventions, performance concerns, security checks).
The CriticsCoordinator gathers all feedback and aggregates the results. It might assign a severity or priority to each critique and then emit a consolidated critique report (plan.critiqued event) back to the PlanCoordinator. By structuring critique as multiple agents, the system is extensible – new critics can be added by registering a new agent without altering the others, thanks to the pub-sub signal design. This also increases robustness: even if one critic fails or times out, the coordinator can proceed with others (perhaps with a warning). The Critic agents essentially serve as the “unit tests” or validators of the plan, ensuring that the proposed tasks make sense and meet all requirements before any execution.
PlanRefiner Agent (Plan Fixer/Refinement)
The PlanRefiner is responsible for improving or correcting the plan based on critic feedback or execution results. When the PlanCoordinator receives critique results indicating flaws or subgoal failures, it invokes the PlanRefiner agent (e.g. via a plan.refine signal) with the current plan and the list of issues identified. The PlanRefiner then works to adjust the plan to resolve these issues. This may involve adding or removing tasks, reordering steps to satisfy dependencies, adjusting task descriptions for clarity, or injecting missing details. Often the refiner will leverage the LLM (through the Engine Manager) to suggest solutions to the problems the critics found – effectively asking “how can we fix these shortcomings in the plan?”. It might use different fix strategies: minimal edits for small issues or a broader redesign for fundamental problems. For example, if a critic flags that “Task B depends on an undefined output of Task A,” the refiner might insert a new subtask in Task A to produce that output, or split Task B into two parts. The PlanRefiner keeps track of modifications and can iterate if multiple rounds of critique are needed. In more advanced usage, the refiner could collaborate with the PlanDecomposer or SubgoalGenerator – e.g. asking for a re-decomposition of a portion of the plan if a major structural flaw is found. After refinement, it emits an updated plan (plan.updated event), which triggers another validation cycle. By isolating the “fixing” logic in its own agent, we ensure the system can autonomously improve plans without human intervention, while keeping the decomposition and critique agents simpler (they just identify issues, the refiner solves them). The PlanRefiner continues to revise the plan until the critics have no more serious complaints or until it reaches a maximum number of iterations.
Spark Planning DSL Interpretation and Usage
RubberDuck’s planning templates are defined declaratively using the Spark DSL (part of the Ash framework). This DSL allows us to describe plan structures (tasks, constraints, etc.) at compile time, providing a blueprint that agents can interpret at runtime. A new RubberDuck.Plan DSL extension (akin to how RubberDuck.Tool is defined) will introduce DSL sections for planning. Specifically, the Planning DSL will have sections such as:
Plan Metadata: high-level attributes like plan name, type/category, description, and perhaps applicable context (similar to plan “metadata” and “type” fields).
Tasks Definition: a declarative list of tasks (and sub-tasks) that comprise the template, each with properties (name, description, maybe tags like complexity or estimated duration). Tasks can be hierarchical or sequential, and support defining dependencies or ordering (e.g., depends_on: ["Task1"]). Each task definition in the DSL might also include a link to a Tool or an Engine handler if execution is automated (for example, a task could specify a tool to run for that step, using the Tool DSL’s definitions).
Constraints: any constraints or global rules for the plan (e.g., deadlines, resource limits, or domain-specific requirements). These can inform both the planning (the PlanDecomposer might take them into account) and the validation (critics will check that constraints are not violated).
Validations: declarations of custom validation rules or critic configurations specific to this plan type. For instance, a plan template could require that “every task must have a success criterion,” or could attach specific critics to run. This serves as built-in quality checks beyond the generic critic agents.
At compile time, Spark DSL definitions are validated and compiled into Elixir modules or data structures (just as Tool definitions are compiled into modules with metadata). A Plan Template Registry can collect these definitions, making templates discoverable by name or type. For example, one might define FeaturePlanTemplate and BugfixPlanTemplate via the DSL; each becomes a known template the PlanCoordinator can use. Using the DSL at runtime: The PlanCoordinator (or the Decomposer) will query the compiled plan template that matches the user’s scenario. This could be done by matching the plan “type” or by an explicit template selection. Once retrieved, the template provides a starting point: it might include a set of predefined tasks or just a scaffold. The PlanDecomposer agent will interpret this template – for example, initializing the plan’s task list with any tasks the DSL defined and marking any template “variables” to fill in. If the template is abstract (like a pattern), the Decomposer uses the LLM to instantiate it. If it’s concrete (like a fixed sequence of known tasks), the Decomposer can directly adopt those tasks and focus on linking them with the user’s specific context. Code Sketch: An illustrative (simplified) example of the Spark DSL for a plan template might look like:
elixir
Copy
defmodule RubberDuck.Planning.FeaturePlanTemplate do 
  use Spark.Dsl, 
    # (Imaginary usage of a planning DSL extension)
    extensions: [RubberDuck.Planning.DSL] 

  plan do 
    name "Feature Implementation Plan"
    type :feature
    description "Steps to implement a new product feature"
  end

  tasks do 
    task "DesignSolution", description: "Design the solution approach"
    task "ImplementFeature", description: "Implement the feature", depends_on: ["DesignSolution"]
    task "WriteTests", description: "Write tests for the feature", depends_on: ["ImplementFeature"]
  end

  constraints do 
    constraint "All tasks must be completed within one sprint"
  end

  validations do 
    # e.g., ensure each task has an associated success metric
    validation "SuccessCriteriaPresent"
  end
end
In this sketch, the DSL defines three ordered tasks for a feature implementation. At runtime, the PlanCoordinator would load FeaturePlanTemplate (perhaps by Planning.Template.fetch(:feature)), and pass its task list to the PlanDecomposer. The PlanDecomposer might then further elaborate these tasks using the LLM (for instance, detailing subtasks under “ImplementFeature” or adjusting wording to the specific feature requested). Constraints and validations defined in the DSL can be automatically fed to the Critic agents – e.g., a ConstraintCriticAgent could read the “one sprint” timeline constraint and verify the plan’s timeline estimates fit it. The declarative nature of the Spark DSL means many plan template errors or omissions are caught at compile time (e.g., undefined dependencies or missing fields), providing compile-time validation of plan structure. Moreover, the DSL can generate documentation for the plan templates and possibly even code for default critic checks, similar to how the Tool DSL generates JSON schemas and docs. In summary, the Spark DSL gives a structured initial plan template or schema which the agents interpret. This bridges human design and AI-driven planning: humans can encode known good planning patterns declaratively, and the agents then fill in the specifics and ensure the plan meets those patterns.
Planning Process and Inter-Agent Collaboration
With roles defined and templates available, the planning execution loop proceeds as a sequence of agent interactions. The agents communicate via Jido signals and messages to hand off tasks and results. The typical workflow is as follows:
Plan Initialization: The PlanCoordinator is invoked with the user’s prompt and context (e.g. via the Engine Manager when a user asks for a high-level solution). It creates a new Plan instance (in memory and/or in the Ash database) and picks a Spark DSL template if applicable. The PlanCoordinator then emits a directive (e.g. a Jido signal plan.decompose) containing the plan context (goal description, selected template, any constraints).
Task Decomposition: The PlanDecomposer agent (listening for plan.decompose events) picks up the signal. It parses the plan template structure provided and integrates it with the user’s specific request. Using LLM-guided reasoning, it breaks the high-level goal into an initial set of tasks and sub-tasks. This may involve multiple steps internally (for example, first generating major tasks, then recursively splitting complex ones). Once decomposition is complete, the PlanDecomposer replies (by emitting a plan.decomposed signal) with the draft plan – essentially a list of tasks with their descriptions, relationships, and any initial ordering or dependency info. This task list is received by the PlanCoordinator, which updates the Plan state (now we have Plan with tasks, but unvalidated). If the PlanDecomposer fails to produce a plan (e.g., the prompt is unclear), the PlanCoordinator might retry with a different strategy or template, or ultimately report an error to the user.
Subgoal Expansion: For each task from the decomposition that still seems too broad or undefined, the PlanCoordinator can trigger the SubgoalGenerator agent. It sends a plan.expand_task signal with the task details. The SubgoalGenerator responds with a set of more granular sub-tasks or steps for that task, via plan.task_expanded events. The PlanCoordinator merges these into the plan, potentially creating a hierarchy (main tasks and nested sub-tasks). This step ensures the plan is detailed enough for execution. (In some implementations, the PlanDecomposer might directly invoke the subgoal agent during step 2 as it encounters complex tasks – the exact coordination can be configured, but ultimately both achieve the hierarchical breakdown.) After this, the plan should contain a structured set of actionable tasks and subgoals.
Critique Phase: The PlanCoordinator now emits a plan.ready_for_review event (or directly calls the CriticsCoordinator agent) to initiate validation. The CriticsCoordinator distributes the current plan to all registered Critic agents (this could be done by broadcasting a plan.critique signal that each CriticAgent subscribes to, or by the coordinator spawning tasks for each critic). Each CriticAgent independently evaluates the plan on its dimension (e.g. checks for missing dependencies, unreachable tasks, violations of coding best practices, etc.). The agents then either respond with a pass (no issues found) or a list of identified issues/suggestions. For example, a DependencyCritic might emit an issue: “Task C depends on Task A’s output, but no task produces that output,” or a SecurityCritic might warn: “No security review task included.” The CriticsCoordinator collects all these responses (with a timeout to avoid waiting indefinitely for any one critic). It aggregates them into a unified critique report and sends that back to the PlanCoordinator (e.g. via plan.critiqued signal with data containing a list of issues and their severities). This parallel critique mechanism ensures a thorough vetting of the plan in minimal time. If absolutely no critics report problems – a rare case for a first draft – the plan may immediately move to finalization.
Plan Refinement: Upon receiving the critique report, the PlanCoordinator determines if the plan needs changes. If critical issues are present (e.g., a task is impossible or a major constraint is unmet), it triggers the PlanRefiner agent by sending a plan.refine message with the plan and issues. The PlanRefiner attempts to modify the plan to resolve all issues. For instance, if critics noted a missing task, the refiner will add it (possibly asking the SubgoalGenerator to flesh it out); if a dependency order was wrong, it will reorder tasks or split them; if a success criterion was undefined, it will define one, etc. The refiner may also use the LLM to generate new text or ideas for tasks (e.g., asking “Suggest a step to address security concerns”). After applying fixes, the PlanRefiner emits an updated plan (plan.updated event). The PlanCoordinator merges these changes into the plan state and increments a revision count. In some cases, the PlanRefiner might not fix everything in one go – it could either partially fix or even possibly introduce new minor issues – so iterative refinement is supported.
Iteration or Completion: The updated plan goes through another cycle of critique (step 4 again) to verify that all previously identified issues were resolved and no new problems emerged. The agents iterate: Critic agents review the revised plan and either give it a green light or report further issues, then the Refiner attempts to fix those. This loop continues until convergence criteria are met. Typically, convergence means either no critical issues remain (the plan passes all critic checks) or the maximum number of iterations has been reached. The PlanCoordinator monitors this. When the plan is deemed complete, the PlanCoordinator marks it as finalized (e.g., status = :complete) and can now hand it off for execution or present it to the user.
Throughout this collaboration, all communication is handled through Jido’s messaging system. Agents do not call each other’s functions directly; instead, they emit and handle signals. For example, PlanDecomposer’s handle_signal/2 callback might match on %Signal{type: "plan.decompose", data: %{...}} and then emit a %Signal{type: "plan.decomposed", data: %{tasks: [...]}} as the result. This decoupling means agents can run on separate processes (or even nodes) and new agents can listen in or join the loop without tight coupling. Handling subgoal failures: Not all plans come together smoothly. If a subgoal or plan section is found to be infeasible (by critics or by a failed execution in a later phase), the agents can collaboratively adjust the plan. For example, suppose during critique the FeasibilityCriticAgent determines that one subgoal “Implement feature X in 1 day” is unrealistic. It would flag this, and the PlanRefiner could respond by splitting that subgoal into smaller daily tasks or allocating more time, then ask the critics to re-evaluate. If even after refinement a particular requirement cannot be met (perhaps a constraint is too strict), the PlanCoordinator has logic to either relax the constraint or escalate (maybe notify the user that the plan can’t satisfy all constraints). The system is designed to try alternative approaches agentically: e.g., the PlanCoordinator might switch decomposition strategies or ask the LLM for a different plan if the initial approach fails completely. Thanks to the modularity, such a change could be as targeted as “only redo subtask planning for Task Y” or as broad as “restart planning with a different template,” depending on which agent is invoked. The use of a persistent Plan state (in memory and optionally stored via Ash) means even if one agent fails, another agent can pick up from the last known state without starting over from scratch.
Autonomy and Convergence Guarantees
In an agentic system with iterative loops, it’s crucial to balance the agents’ autonomy (their ability to keep improving the plan on their own) with guarantees of convergence (avoiding infinite loops or oscillations). Our design addresses this in several ways:
PlanCoordinator as Moderator: The PlanCoordinator agent ensures the loop converges by imposing iteration limits and convergence checks. It can maintain a counter of refinement cycles and stop after, say, N iterations to prevent infinite back-and-forth. For example, if after 3 refine→critique cycles the critics are still flagging issues, the PlanCoordinator might conclude the plan is “good enough” or cannot be improved further without external input. It could then finalize the plan with warnings or ask for human confirmation. This prevents unbounded loops of agents critiquing each other’s changes endlessly.
Convergence Criteria: Apart from a hard iteration limit, the system can check if the changes are diminishing. The PlanCoordinator can compare the critique reports between iterations – if the set of issues is not shrinking or if it oscillates (issue A fixed, but issue B reappears, etc.), it detects lack of progress. Additionally, each issue can have a severity; the loop might stop when only low-severity suggestions remain (assuming those can be optional improvements). This way, agents have freedom to iterate, but we have a clear condition to declare “done.”
Time and Resource Bounds: Each agent’s actions are time-bounded to ensure responsiveness. For instance, if a CriticAgent does not respond within a timeout (perhaps due to external resource lag), the PlanCoordinator can proceed without it or trigger a fallback. Similarly, LLM calls in decomposition/refinement are given a budget (in terms of tokens or time). This prevents the autonomous loop from hanging on a single step. The system can log and gracefully skip delayed agents to keep momentum.
Deterministic Protocol: The agent interaction protocol itself is structured to converge. The phases occur in a logical order (decompose → critique → refine → repeat) with no circular dependencies between agents beyond the intended loop. There is no deadlock because each agent awaits a specific input signal to act and then produces an output. By designing the signal flow carefully (no agent waits indefinitely for another except the coordinator aggregating results with timeouts), we avoid livelock. Essentially, PlanCoordinator acts like a state machine driving the process to a terminal state.
Minimal Autonomy Necessary: Each agent is autonomous in how it performs its function (e.g., the CriticAgent autonomously decides if the plan is good or not on its dimension), but they do not autonomously decide to prolong the process outside the protocol. The Critic will report and stop; the Refiner will propose a change and stop. They don’t continuously act unless prompted by a new signal from the coordinator. This pattern ensures we don’t have free-roaming agents making unrelated changes. Autonomy is used to improve modularity and expertise (each agent is an expert in something) rather than to create an uncontrollable system. In essence, the autonomy is bounded by the collaborative protocol defined by the planning loop, which is guaranteed to terminate under the above conditions.
By combining these measures, the system ensures it will converge on a plan in a finite number of steps, or gracefully abort with an explanation, rather than endlessly churn. This provides confidence that letting the agents “run loose” on a problem won’t result in chaos – they have the freedom to iterate and be creative, but within a supervised framework that guarantees a conclusion.
OTP Architecture and Supervision Strategy
The entire multi-agent planning system is implemented using Elixir’s OTP framework, leveraging processes and supervisors for reliability. Each Jido agent corresponds to an OTP GenServer process (likely implementing a RubberDuck.Agents.BaseAgent behavior that defines standard callbacks). We design a supervision tree to manage these agents and the planning workflows:
Agent Supervision Tree: We introduce a top-level supervisor (e.g., RubberDuck.Agents.PlanningSupervisor) under the main application. The PlanningSupervisor can spawn a dynamic supervision subtree per active plan. For each new planning session, the PlanCoordinator process is started as a supervisor (or as a supervised worker that can spawn children). Under a given PlanCoordinator, we either spawn dedicated instances of the other agents (Decomposer, SubgoalGenerator, Refiner, etc.) or use worker processes/tasks as needed. A simple approach is to spawn one set of agents per plan for full isolation. In that case, when PlanCoordinator starts, it could dynamically start a linked PlanDecomposer, a PlanRefiner, and possibly a CriticsCoordinator (which in turn may spawn individual Critic workers or processes). These child agents would all be linked to the PlanCoordinator’s supervisor, using a :rest_for_one or :one_for_one strategy depending on desired behavior. For example, if the PlanCoordinator crashes, we might want to terminate the whole plan’s agents (so rest_for_one could propagate a restart). Alternatively, if a CriticAgent crashes, we might simply log it and continue without that critic (one_for_one ensures other agents continue). Each agent process is given a clear termination condition: e.g., when the plan is finished or aborted, the PlanCoordinator can terminate its children and then itself (or be terminated by its supervisor). This design means no agent process lives indefinitely for no reason – they exist only for the lifespan of planning jobs, avoiding resource leaks.
Alternate Pooling Approach: Optionally, some agents could be implemented as global pools rather than per-plan. For instance, CriticAgents might be long-running services that handle critique requests for any plan (this avoids spawning multiple identical critics). In that model, the Critic agents are started at application boot (supervised globally) and simply subscribe to plan events, differentiating plans by an ID in the signal payload. The advantage is reusability and warm caches (e.g., a SecurityCriticAgent might load a vulnerability database once). The disadvantage is that the agent must handle concurrent requests for different plans. We can mitigate that by having each CriticAgent spawn a short-lived worker (Task) for each plan critique if needed. Both designs (per-plan vs. shared) are viable; the choice can be based on expected load. Given RubberDuck’s likely moderate concurrency, a per-plan isolated agent set provides simplicity in reasoning (each plan is a self-contained mini-supervisor with its agents).
BaseAgent Behavior: All agents implement the RubberDuck.Agents.BaseAgent callbacks, which unify their interface to the Jido runtime. This means each agent defines an init/1 to set up state (like storing a reference to the current plan or plan ID) and a handle_signal/2 to respond to relevant messages. For example, the PlanDecomposer’s handle_signal would pattern-match on a %Signal{type: "plan.decompose", data: %{"plan_id" => id, ...}} and ignore other signals. The BaseAgent behaviour ensures a consistent pattern for error handling and logging across agents. Common functionalities – like emitting signals, subscribing to topics, persisting partial state – can be provided as helper functions in the BaseAgent module for all to use.
Fault Tolerance: OTP supervision means if an agent crashes (due to an exception or an ill-behaved LLM response causing an error), it can be automatically restarted in a clean state. For instance, if the PlanDecomposer agent hits an unexpected error while parsing a template, the supervisor will restart it, and the PlanCoordinator can resend the plan.decompose message (perhaps with a simpler strategy or after sanitizing input). Similarly, if the PlanRefiner crashes (maybe a bug in how it applied a fix), it restarts and could either attempt the refinement again or let the PlanCoordinator decide to abort. Because state is largely kept in the PlanCoordinator (and the plan data is persisted via Ash or in memory), a crashed agent can often resume work without losing critical information. We will implement appropriate back-off and retry limits to avoid constant thrashing (OTP allows specifying max restart intensity).
Ordering and Synchronization: In OTP, each GenServer agent processes one message at a time, which simplifies reasoning about each agent’s internal state. The PlanCoordinator can use synchronous calls (GenServer.call) or just fire-and-forget signals depending on the need to block for results. For example, it might use call to ask PlanDecomposer for results (making the decomposition step synchronous from the coordinator’s perspective), but use asynchronous cast or signals to launch critiques in parallel. Under the hood, Jido’s signal routing uses processes and possibly a router process to deliver events to subscribers. Our design will ensure that, for each plan, signals carry a plan_id so that even if multiple plans are in flight, the agents can filter out irrelevant signals (either by subscription filtering or in handle_signal logic).
Monitoring and Introspection: Since this is an OTP system, we can integrate with standard tools to monitor processes. Each plan’s coordinator and agents can emit telemetry events for steps (e.g., an event when decomposition finishes, when a critique round completes, etc.). We could incorporate an OTP :observer or Phoenix LiveDashboard integration for debugging agents. Additionally, Jido’s own development tools (if any) could allow us to inspect active signals and agent statuses. The Engine Manager can also track these processes if needed (for instance, correlating an engine request to a planning process).
Ash Persistence: If we use the Ash Framework for plan storage (as in Phase 7 where Plan and Task are Ash resources), the PlanCoordinator can perform database writes at milestones (initial plan created, final plan saved, etc.). This ensures that if the system crashes mid-way or needs to resume, we have a record of what tasks were planned or which iteration it was on. However, during the live planning loop, agents will primarily work with in-memory state for speed, and only checkpoint to the DB occasionally (to avoid slowing down each agent step with DB I/O). The OTP supervision ensures either the whole plan completes or we can roll back partial results cleanly if the process is terminated.
Overall, the OTP architecture provides robustness: each agent is isolated (a failure in a CriticAgent won’t crash the Decomposer), and supervision can recover from errors automatically. By structuring agents under a dynamic supervisor per plan, we also ensure that once a plan is done, all associated processes can be cleanly torn down, freeing resources. This design aligns with the planned Jido integration for RubberDuck, which emphasizes converting monolithic logic into supervised autonomous agents.
Communication Patterns Between Agents
We employ a hybrid communication strategy using Jido’s messaging capabilities, combining publish/subscribe events with directed messages to achieve both flexibility and control:
Publish/Subscribe via Signals: Many interactions use a pub-sub model where agents listen for certain signal types. Jido’s CloudEvents-based signal router allows agents to subscribe with wildcard patterns and receive relevant events. We leverage this for phases where multiple agents need the same input. For example, the PlanCoordinator’s announcement plan.ready_for_review can be a broadcast – all CriticAgents subscribe to "plan.ready_for_review" (optionally filtered by plan_id) and will simultaneously receive the plan data to critique. This decouples the coordinator from knowing how many critics exist or which ones; new CriticAgents can be added by simply subscribing to that event type. Similarly, if we had multiple different decomposition agents (say different strategies), the coordinator could broadcast a plan.request_decomp_strategy and any decomposition agent could race to provide a plan (though in our design we use one decomposition agent with internal strategy selection). Pub-sub is ideal for one-to-many communication and for building a scalable, extensible system where agents operate more like independent services.
Directed Requests (Goal-Directed Messaging): In some cases, a specific agent needs to be invoked with a particular goal. For instance, when the PlanCoordinator needs a refinement, it specifically calls the PlanRefiner agent. In a pub-sub world, it could emit plan.refine and only the Refiner is subscribed to that, which is effectively the same as a direct message. However, Jido may also support addressing a message to a particular agent or using a unique signal topic (like "plan.refine.plan123" where only that plan’s refiner listens). We use directed messaging for one-to-one interactions such as PlanCoordinator -> PlanDecomposer (only one decomposer agent responds per plan) and PlanCoordinator -> PlanRefiner. This ensures clarity in the flow of control – the coordinator knows which agent will handle the signal. Directed messages (or targeted signals) also simplify correlating responses. For example, if the PlanCoordinator sends a plan.decompose with a specific request ID, the PlanDecomposer can include that ID in the plan.decomposed response so the coordinator knows it’s the answer to its request.
Blackboard Model (Shared State): Our design doesn’t rely on a separate blackboard component, but we do emulate aspects of the blackboard model via the PlanCoordinator’s state and the Ash plan record. The PlanCoordinator essentially acts as a central data store (maintaining the authoritative copy of the plan). Agents do not directly modify the global plan; instead, they suggest modifications via signals, and the PlanCoordinator updates the shared plan state. In effect, this is like a moderated blackboard: the plan is the blackboard, PlanCoordinator is the moderator that writes to it, and other agents read from it (the plan is included in signals or fetched from the coordinator). An alternative design could have been to use an ETS table or an Ash resource as a blackboard that all agents read/write. However, that approach can introduce race conditions and complexity in synchronization. By funneling changes through the PlanCoordinator, we get a single source of truth and serialization of updates. We still achieve the loose coupling of a blackboard (agents don’t call each other directly for data; they consult the plan state), but with the safety of a controlled mediator.
Message Content and Format: We use structured events with clear types (as seen above: plan.decompose, plan.decomposed, plan.critiqued, etc.). Each signal’s data payload includes the plan_id (so global critics can filter by the specific plan they should handle), and any other necessary info (list of tasks, list of issues, etc.). This design is aligned with CloudEvents conventions used by Jido – each event has a type, source, data, and possibly correlation IDs. By adhering to a convention (prefixing all planning events with plan. and including plan_id), we make the system debuggable and extensible (other agents or logging components could listen to plan.* events for monitoring or auditing without interfering).
Coordination Pattern: The combination of pub-sub and directed messaging essentially implements a mediator pattern with event broadcasting. The PlanCoordinator is the mediator that initiates phases and also the collector of results, but the heavy lifting in each phase is done by independent agents who communicate via events. We avoid a complex tangle of every agent talking to every other arbitrarily; instead communication is structured primarily as hub-and-spoke (Coordinator-centric) with one notable exception: the CriticAgents talk only to the CriticsCoordinator (or effectively to the PlanCoordinator via the coordinator aggregating them). In effect, the PlanCoordinator and CriticsCoordinator implement a two-level mediation: PlanCoordinator handles sequential phase transitions, and CriticsCoordinator handles parallel distribution among critics. This keeps interactions orderly.
Parallelism and Ordering: Our messaging ensures that phases that can be parallelized are parallelized (all critics run together upon one event), while inherently sequential phases are kept in order by the coordinator’s logic. The system doesn’t rely on any global lock or sequential bottleneck except where necessary (e.g., we wouldn’t start critique before decomposition is done, because the coordinator won’t emit ready_for_review until it has received decomposed). Within a critique cycle, however, each CriticAgent works concurrently on the same plan data. They might send back results in arbitrary order; the CriticsCoordinator will collect them, and only when all have responded (or timed-out) will it forward the aggregate result – thereby reintroducing synchronization at the end of the parallel phase. This approach maximizes concurrency without sacrificing the determinism of each loop iteration.
In summary, the communication strategy uses pub-sub for flexibility and extensibility, and directed signals for clarity in one-to-one tasks. The PlanCoordinator’s maintained state provides a controlled blackboard-like memory. This mixed model plays to the strengths of Jido’s messaging (easy broadcast and filtering) and OTP’s process addressing (direct calls) to create a robust yet nimble coordination mechanism. Notably, this design makes it easy to plug in new agents: for example, if we develop a new “OptimizationCriticAgent” that suggests more efficient task orders, we can simply subscribe it to plan.ready_for_review and it will start receiving plans and contributing feedback without any change to existing agents. The rest of the system will treat its feedback just as any other critique.
Integration with Existing RubberDuck Architecture
The new Jido-based planning system is designed to smoothly integrate with RubberDuck’s architecture. We reuse and interface with components like the Engine Manager, Tool DSL, and Workflow Orchestrator so that planning becomes an integrated part of the assistant’s workflow rather than a silo. Here’s how each is incorporated:
Engine Manager Integration
RubberDuck’s Engine Manager manages interactions with various LLM engines and reasoning subsystems. The planning agents will leverage it for any heavy LLM-driven tasks. Specifically, the PlanDecomposer and PlanRefiner agents interface with the Engine Manager to perform natural language reasoning and generation. For example, when the PlanDecomposer needs to generate a list of tasks from a high-level prompt, it may call out to an LLM (like GPT-4) possibly using a chain-of-thought prompt to ensure thorough breakdown. Instead of these agents making raw API calls, they issue a request to Engine Manager (which already knows how to utilize the configured LLM, whether locally or via API). This means the agents get access to advanced prompting techniques and multi-step reasoning flows that the Engine Manager supports. In fact, the previous implementation included a Chain-of-Thought module and prompt templates; the PlanDecomposer can call those to improve its output. Similarly, the PlanRefiner may use the Engine Manager to ask the LLM for suggestions on fixing plan issues (“Given this plan and these problems, suggest improvements”). By funneling through Engine Manager, we also respect user settings (like which model to use, how many tokens, etc.) and benefit from any caching or rate-limiting it provides. Beyond LLM calls, Engine Manager might also manage other “engines” – perhaps a vector search or knowledge base. If planning needs to, say, retrieve documentation or examples (for instance, to validate if a task is feasible), an agent could ask Engine Manager to perform a relevant retrieval-augmented query. The key is that Engine Manager becomes the toolbelt for agents, allowing them to invoke sophisticated AI capabilities without hardcoding any API logic. The integration is straightforward: an agent can be passed a reference to Engine Manager (or access it via a global interface) and then call something like EngineManager.request(engine_name, input) from within the agent’s handler. The result can be awaited synchronously or delivered via a callback message. We will ensure that such calls are done asynchronously within the agent (so as not to block the agent’s process – for example, using Task.await or spawning a temporary Task), since LLM calls can be slow. The OTP design handles this well (the agent can handle other signals or heartbeats while waiting for the LLM). In summary, the planning system depends on Engine Manager to provide the “intelligence” behind planning – the creative task breakdown and refinement suggestions – while the agents provide the structure and coordination. This reuse of Engine Manager avoids duplicating LLM integration code and ensures consistency with how other parts of RubberDuck (like the conversational engine) operate.
Tool DSL and Tool Execution Integration
RubberDuck’s Tool DSL (built with Spark in Phase 9) provides a declarative way to define tools (external actions, commands, or functions that the AI can invoke). The planning system will integrate with this in a couple of ways:
Plan Steps Referring to Tools: Many coding assistant plans involve actions that can be automated by tools. For example, a plan might have a task “Run Tests” or “Format Code” – tasks that correspond to actual operations rather than creative work. In our design, tasks can be linked to Tool definitions. The Spark Planning DSL could allow a task to specify a :tool attribute or similar, referencing a tool name defined via RubberDuck.Tool DSL. If the plan includes such a task, the system knows that when executing the plan, it should invoke that tool. Even during planning, the presence of a tool reference is useful: the CriticAgents can verify tool availability or check that inputs/outputs of that task align with the tool’s specification (since the Tool DSL provides a schema, the critic can catch if the plan is trying to feed wrong data to a tool). The PlanCoordinator or PlanExecutor (if a separate execution phase exists) can lookup the tool in the Tool Registry at runtime to run it. Integrating at the DSL level means planning and tooling speak the same language – e.g., if a tool is defined for running linters, the plan can include a “Run linter” step and the system will automatically know how to perform it when the time comes.
Agents Using Tools Internally: Agents themselves might use tools to gather information during planning. For instance, a SecurityCriticAgent could utilize a static analysis tool (if one exists in the Tool registry) to scan the codebase for vulnerabilities as part of validating the plan’s sufficiency (perhaps out of scope for planning, but conceivable in later execution validation). Another example: the PlanRefiner might use a “diff tool” to compare plan versions or a “cost estimator” tool to gauge timeline feasibility. If such tools are registered, agents can invoke them through the standardized Tool API (which handles permissions, sandboxing, etc.). This ensures the agents don’t execute arbitrary shell commands or unsafe operations; they only call pre-approved tools with the Engine Manager or Tool Registry mediating, preserving security and safety.
Tool Suggestions: The planning agents, especially the LLM-driven ones, might suggest using certain tools if they know they exist. Because the Tool DSL allows us to introspect available tools, the PlanDecomposer could be designed to automatically include a task if a relevant tool exists. For example, if the plan is for “improving code quality,” the PlanDecomposer might check the Tool Registry for any code quality analyzers and then insert a task “Run Code Quality Analyzer” tied to that tool. This synergy means the AI planning is aware of the assistant’s capabilities and can produce actionable plans that utilize those capabilities, rather than purely abstract suggestions.
Implementation-wise, integrating the Tool DSL is straightforward: the PlanCoordinator and PlanDecomposer will have access to the RubberDuck.Tool.Registry to query tools. Task definitions in the Spark Planning DSL can include tool linkage which is validated at compile time (ensuring no plan references a non-existent tool). Then, at runtime, if a plan task is flagged as a “tool” task, the PlanCoordinator can mark it for automatic execution by the Workflow Orchestrator or PlanExecutor. Thus, the planning system extends the utility of the Tool DSL into the strategic domain: not only do we have tools defined, but we have agents that know how to plan the use of those tools in a larger context. This yields plans that are directly executable by RubberDuck’s tool infrastructure, closing the loop between planning and action.
Workflow Orchestrator Integration
Prior to this agentic redesign, RubberDuck had a Workflow Orchestrator that coordinated multi-step AI behaviors (like plan-then-act sequences). With the new planning system, much of the orchestration logic for planning is now encapsulated in the agents and their messaging. However, integration (and possibly eventual replacement) of the Workflow Orchestrator is important for a seamless architecture:
Invocation: The Workflow Orchestrator (if it’s still the component managing high-level user interactions) can invoke the planning loop as a sub-workflow. For example, when the user asks “Implement feature X,” the orchestrator might determine that the PlanningEngine (now our PlanCoordinator/agents) should handle it. In that case, it calls the PlanCoordinator (perhaps via an Engine Manager interface or a simple message) and awaits the result. In a sense, the PlanCoordinator becomes a new “engine” or routine that the orchestrator can call in its scripts. The orchestrator doesn’t need to micro-manage the steps of planning, since the agents do that; it just triggers the process and then proceeds once a plan is ready.
Replacement by Agents: In the long run, we can represent the planning workflow itself as a workflow in Jido’s terms. The Jido framework includes a Workflow engine and DSL to define sequences of agent interactions. We could formalize the planning loop (steps 1–6 described above) as a workflow template using that system. This would allow the orchestration to be handled by Jido’s engine, orchestrating the signals between agents according to a defined graph. If this is done, it might supersede the old Workflow Orchestrator for the planning aspect, since Jido would be handling the coordination. Our design is in fact already a specialized workflow (decompose → critique → refine loop), so porting it to the Workflow DSL would be natural. This aligns with the Phase 15 plan to integrate a Workflow Engine with Jido.
Coexistence: While transitioning, the agent system coexists with the orchestrator. The orchestrator focuses on other concerns (like orchestrating user chat, file edits, etc.), and when it needs a project plan, it delegates to the agent system. Communication between them can be through Elixir messages or an internal API. For example, the orchestrator could call RubberDuck.Planning.plan_for_prompt(prompt, context) which internally starts a PlanCoordinator and returns the finalized plan. The orchestrator would then possibly trigger an execution phase using that plan (e.g., by sending it to the PlanExecutor or scheduling tasks).
State Sharing: The orchestrator or other parts of the system might need to know intermediate planning status for UI updates (say, to show progress to the user). We can tap into the status messaging system (Phase 10’s Real-Time Status updates). The PlanCoordinator can broadcast status events (via Phoenix Channels or similar) during each phase – e.g., “Decomposing plan…”, “Critiquing plan (3 issues found)…”, etc. This would integrate with any LiveView or frontend to keep the user informed. The Workflow Orchestrator (which likely already hooks into Phoenix Channel updates) would not treat planning as a black box but subscribe to these events and forward them appropriately to the UI.
Plan Execution: Once a plan is finalized, the orchestrator or a dedicated PlanExecutor will carry out the tasks (possibly with ReAct style interleaving of thinking and acting). The planning system’s output is designed to feed directly into execution: a structured plan with tasks that map to either LLM actions or tool invocations. The existing ReAct Execution framework (Phase 7.4) can take the plan and execute tasks sequentially or in parallel, observing results and adapting if needed. Our agentic planning integrates with that by providing a plan that meets all the pre-checks (thanks to the critics). If during execution some unexpected issue occurs, it could even trigger the planning agents again (for example, if an execution fails, maybe call PlanRefiner to adjust the plan). This tight integration ensures the system is closed-loop: plan, act, and re-plan if needed, all under a unified architecture.
In conclusion, the planning system is not an island – it deliberately builds on RubberDuck’s Engine Manager for AI reasoning, uses the Tool DSL to make plans executable and safe, and works within the Workflow Orchestrator/Engine to fit into the overall user request handling. By designing the agents and DSL within the Ash/Spark ecosystem, we maintain consistency with the rest of the codebase (e.g., using Ash resources for persistence, Spark DSL for declarations, OTP for concurrency). The result is a planning system that is modular, intelligent, and cooperative: agents with specialized skills interpreting declarative templates and working together via Jido signals to produce a reliable plan, all while being supervised for robustness and integrated for usability. This fulfills the goal of transforming RubberDuck’s planning into an agent-based paradigm, yielding a system that is easier to scale, extend, and maintain.
